\documentclass{standalone}
% Preamble
\begin{document}


\section{Multivariate case}
\label{multivariate}

For a univariate polynomial $f$, the structure of the quotient algebra $A$ consists of the monomial basis and the companion matrix; this matrix is obtained either by a direct reading of the coefficients of $f$, or by taking the ratio of the two matrices $B(1), B(x)$; in contrast, for a multivariate polynomial system, neither a basis nor the companion matrices (matrices of the multiplication maps
$x_j : \left\vert
\begin{array}{c}
h \mapsto x_jh
\end{array}
\right.$ in the basis) can be read from the coefficients of the given polynomials. It is easy, however, to construct the Bezout matrices $B(1), B(x_1), \cdots, B(x_n)$, from which one can derive a basis of $A$ and the related companion matrices $X_1,\cdots, X_n$. 
Let's set the framework: given $n$ polynomials $f_1,\cdots, f_n$ in the variables $x_1,\cdots, x_n$, with coefficients in $\mathbb{Q}$, we denote by
\begin{itemize}
\item $\mathbb{Q}[x]$ the ring of polynomials in the variables $x = x_1,\cdots, x_n$,
\item $\langle f \rangle$ the idal generated by $f = f_1,\cdots, f_n$,
\item $A = \mathbb{Q}[x]/\langle f\rangle$ the quotient algebra.
\end{itemize}
From now on we assume that $\langle f\rangle$ is {\bf zero-dimensional}, that is to say the vector space $A$ is finite dimensional \cite[p.~234]{clo}. This is always the case when $n = 1$.

\subsection{Construction of Bezout polynomials and Bezout matrices}

\subsubsection{Extension of Definition \ref{def_bez} to the multivariate case}

\begin{defn}
Let $x^\gamma = x_1^{\gamma_1}\cdots x_n^{\gamma_n} \in \mathbb{Q}[x]$ be some monomial.
We introduce a new variable set $y = y_1,\cdots, y_n$ and consider, for each couple of indices $i, j = 1\cdots n$, the ratio
\begin{equation}
\label{finite_diff}
\delta_{i,j}(x^\gamma) = \dfrac{y_j^{\gamma_j}f_i(y_1,\cdots, y_{j-1},x_j,\cdots,x_n) - x_j^{\gamma_j}f_i(y_1,\cdots,y_j,x_{j+1},\cdots,x_n)}{x_j - y_j}
\end{equation}
which is a polynomial in the variables $x, y$. 
With the $\delta_{i,j}$'s we form a finite difference matrix
\begin{equation}
\label{Delta}
\Delta(x^\gamma) = (\delta_{ij}(x^\gamma))_{ij}
\end{equation}
whose determinant $\delta(x^\gamma) = det(\Delta(x^\gamma))$, a polynomial in both variables $x, y$, is called the {\bf Bezout polynomial}, or {\bf bezoutian}, of $x^\gamma$.
This definition can be extended by linearity to a more general polynomial $g = \sum_\gamma g_\gamma x^\gamma \in \mathbb{Q}[x]$ by the formula
$$\delta(g) = \sum_\gamma g_\gamma \delta(x^\gamma)$$
Writting $\delta(g)$ as a sum of monomials, 
\begin{equation}
\label{def_bez}
\delta(g) = \sum_{0 \le \alpha,\beta} b_{\alpha\beta} x^\alpha y^\beta
\end{equation} 
we define $B(g)$, the {\bf Bezout matrix} of $g$, as the matrix of the coefficients $B(g) = [b_{\alpha\beta}]$. If we denote by $\bold{x}$ and $\bold{y}$ the sets of all the monomials $x^\alpha$ et $y^\beta$ that appear in~(\ref{def_bez}), then we have the relation, similar to~(\ref{xBg})
\begin{equation}
	\delta(g) = \bold{x} B(g) \bold{y}^T
\end{equation}
\end{defn}
The following example, see \cite{jpc}, illustrates the previous definitions

\begin{exmp}
\label{bez_multi}
We take $n = 2$, $f_1 = x_1^2 + x_1x_2^2 - 1, f_2 = x_1^2x_2 + x_1$ and want to calculate the Bezout matrices $B(1), B(x_1), B(x_2)$, which are useful for computing the companion matrices $X_1, X_2$. To begin with, we compute the finite difference matrices, as defined in (\ref{Delta})
\begin{align}
\Delta(1) &=
\begin{pmatrix}
x_1 + x_2^2 + y_1 & x_2y_1 + y_1y_2 \\
1 + x_1x_2 + x_2y_1 & y_1^2
\end{pmatrix} \nonumber  \\
\Delta(x_1) &=
\begin{pmatrix}
1 + x_1y_1 & x_2y_1 + y_1y_2 \\
1 + x_1x_2 + x_2y_1 & y_1^2
\end{pmatrix} \nonumber  \\
\Delta(x_2) &=
\begin{pmatrix}
x_1 + x_2^2 + y_1 & 1 - y_1^2 + x_2y_1y_2 \\
1 + x_1x_2 + x_2y_1  & -y_1
\end{pmatrix} \nonumber
\end{align}
whose determinants are the bezoutians
\begin{align}
\delta(1) &= -x_2y_1 - x_1x_2^2y_1 + x_1y_1^2 + y_1^3 - y_1y_2 - x_1x_2y_1y_2 - x_2y_1^2y_2 \nonumber \\
\delta(x_1) &=  y_1^2 - x_1x_2^2y_1^2 + x_1y_1^3 - x_1x_2y_1^2y_2 \nonumber \\
\delta(x_2) &= -1 - x_1x_2 - x_1y_1 -x_2y_1 - x_2^2y_1 + x_1x_2y_1^2 + x_2y_1^3 - x_2y_1y_2 - x_1x_2^2y_1y_2 - x_2^2y_1^2y_2\nonumber
\end{align}
The monomial appearing in these polynomials are
$\bold{x} = (1, x_2, x_2^2, x_1, x_1x_2, x_1x_2^2)$ and $\bold{y} = (1, y_1, y_1y_2, y_1^2, y_1^2y_2, y_1^3)$; the Bezout matrices $B(1), B(x_1), B(x_2)$ appear when we write these bezoutians as double-entry arrays indexed by $\bold{x}, \bold{y}$
$$\begin{array}{c|cccccc}
	\delta(1) & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  & -1 &  &  & 1\\
	x_2 &  & -1 &  &  & -1 & \\
	x_2^2 &  &  &  &  &  & \\
	x_1 &  &  &  & 1 &  & \\
	x_1x_2 &  &  & -1 &  &  & \\
	x_1x_2^2 &  & -1 &  &  &  &
\end{array}$$
$$\begin{array}{c|cccccc}
	\delta(x_1) & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  &  & 1 &  & \\
	x_2 &  &  &  &  &  & \\
	x_2^2 &  &  &  &  &  & \\
	x_1 &  &  &  &  &  & 1\\
	x_1x_2 &  &  &  &  & -1 & \\
	x_1x_2^2 &  &  &  & -1 &  &
\end{array}$$
$$\begin{array}{c|cccccc}
	\delta(x_2) & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 & -1 &  &  &  &  & \\
	x_2 &  & -1 & -1 &  &  & 1\\
	x_2^2 &  & -1 &  &  & -1 & \\
	x_1 &  & -1 &  &  &  & \\
	x_1x_2 & -1 &  &  & 1 &  & \\
	x_1x_2^2 &  &  & -1 &  &  &
\end{array}$$

\end{exmp}

\begin{rem}
Contrasting with the univariate case, $\bold{x}$ and $\bold{y}$ are not bases of the vector space $A$. 
But, as we will see, they are  generating sets and we will examine how to build bases from them.
\end{rem}

\subsubsection{Practical computation of the Bezout matrices}
In the previous example, the matrices $\Delta(1), \Delta(x_1), \Delta(x_2)$ have size $2$ and their entries are polynomials in $x_1, x_2$; calculating their determinant is easy. But, if either the number  $n$ of variables or the degree of the input polynomials $f_i$ increase, then this calculation becomes impractical because one cannot use the Gauss pivot algorithm to a polynomial entries matrix. We can however overcome this difficulty by applying the following evaluation-interpolation process :
\begin{enumerate}
\item
Estimate a priori the set of monomials $x^\alpha y^\beta$ appearing in the Bezout polynomial $\delta(x_k)$ (for fixed $k$, $k = 0,\cdots, n$, with the convention that $x_0 = 1$),
\item
Evaluate the polynomial-entries matrix $\Delta(x_k)$ on all pairs $(u, v)$ where $u = (u_1,\cdots, u_n) \in U$ and $v = (v_1,\cdots, v_n) \in V$, $U, V$ being adequate sets of Fourier multi-points,
\item
For each $(u, v) \in U\times V$, apply the Gauss pivot method to the numerical matrix $\Delta(x_k)(u, v)$ in order to compute its determinant $\delta(x_k)(u, v)$,
\item
Interpolate the set of numerical values $\delta(x_k)(u, v)$ in order to obtain the polynomial $\delta(x_k)$.
\end{enumerate}

Let us specify, in the process above, which are the monomials appearing in $\delta(x_k)$ - item 1 - and which are the Fourier points used to evaluate $\delta(x_k)$ - item 2. Suppose that the polynomial system $f$ has multi-degree $(d_1, \cdots, d_n)$, that is to say the partial degree of each $f_i$, in the variable $x_j$, is smaller or equal to $d_j$. One can see, then, that the polynomial $\delta(x_k)$ has multi-degree $(d_1, 2d_2, \cdots, nd_n)$ in the variable $x$ and multi-degree $(nd_1, (n-1)d_2, \cdots, d_n)$ in the variable $y$. To evaluate $\delta(x_k)$, we choose the Fourier set $U = \prod_{j=1..n} U_j$ where $U_j$ is the set of complex roots of $X^{jd_j} - 1$; and we choose $V = \prod_{j=1..n} V_j$ in such a way that $U_j$ et $V_j$ are disjoint sets, so that the denominator of the ratio (\ref{finite_diff}) never vanishes. This is obtained, for example, if $V_j$ is the set of complex roots of $X^{(n-j+1)d_j} - \theta_j$ with $\theta_j = e^{i\pi/j}$ (here $i$ means the square root of $-1$). 


These considerations lead to the following algorithm providing the sets $U$ and $V$.


\begin{algorithm}[]
\label{fourierPoints}
\KwData{$d = (d_1, \cdots, d_n)$, multi-degree of polynomial system}
\KwResult{$U, V$, two sets of Fourier points}
\For{$j = 1,\cdots, n$}{
	$U_j \gets$ roots of $X^{jd_j}-1$\;
	$V_j \gets$ roots of $X^{(n-j+1)d_j}-e^{i\pi/j}$\;
}
$U \gets \prod_{j=1..n}U_j$\;
$V \gets \prod_{j=1..n}V_j$\;
\caption{Construction of $U, V$, two sets of Fourier points}
\end{algorithm}

Then, we evaluate the bezoutian $\delta(x_k)$ on the Fourier points $(u, v) \in U\times V$

\begin{algorithm}[]
\label{algo_2}
\KwData{polynomial sytem $f$, index $k$}
\KwResult{matrix $C^{(k)}$ containing the evaluations $\delta(x_k)(u, v)$}
$(d_1, \cdots, d_n) \gets$ multi-degree of $f$\;
Get $U, V$ via Algorithm \ref{fourierPoints}\;
$D \gets \prod_{j=1..n}jd_j$\;
$C^{(k)} \gets \textsc{zeros}(D, D)$\;
\For{$(u, v) \in U\times V$}{
	$\Delta \gets \textsc{zeros}(n, n)$\;
	\For{$i, j=1..n$}{
		$\Delta_{i,j} \gets \delta_{i,j}(x_k)(u, v)$
	}
	$C^{(k)}_{u, v} \gets \textsc{det}(\Delta)$
}
\caption{Evaluation of the bezoutian $\delta(x_k)$ on $U\times V$}
\end{algorithm}

Let us show, to conclude, how the Bezout matrix $B(x_k)$ is simply related to $C^{(k)}$.
To simplify, we denote the bezoutian $\delta(x_k)$ by $\delta^{(k)}$ and the Bezout matrix $B(x_k)$ by $B^{(k)}$; recall that $C^{(k)}$ denotes the evaluation matrix of $\delta^(k)$ on $U\times V$.
The matrix $B^{(k)} = \left[b^{(k)}_{\alpha\beta}\right]$ satisfies $\delta^{(k)}(x, y) = \sum_{\alpha,\beta} b^{(k)}_{\alpha\beta} x^\alpha y^\beta$,
thus $C^{(k)}_{u,v} = \delta^{(k)}(u, v) = \sum_{\alpha,\beta} b^{(k)}_{\alpha\beta} u^\alpha v^\beta$; this writes as a matrix product
$\left[C^{(k)}_{u,v}\right]_{u,v} = \left[u^\alpha\right]_{u,\alpha} \left[b^{(k)}_{\alpha,\beta}\right]_{\alpha, \beta} \left[v^\beta\right]_{v, \beta}^T$. If we define the Fourier matrices $F_u = \left[ u^\alpha \right]_{u, \alpha}$ and $F_v = \left[ v^\beta \right]_{v, \beta}$, then we get the evaluation-interpolation relation between matrices $B^{(k)}$ and $C^{(k)}$
\begin{equation}
C^{(k)} = F_uB^{(k)} F_v^T
 \end{equation}
Since $U$ and $V$ consist of Fourier points, $F_u$ et $F_v$ are unitary and $B^{(k)}$ writes as the matrix product
 \begin{equation}
 B^{(k)} = F_u^*C^{(k)} \overline{F_v}
 \end{equation}
The computation of the Bezout matrices, as described above, have been implemented in Numpy and can be found at \cite{jp_code}.

\end{document}
