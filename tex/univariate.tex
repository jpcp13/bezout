\documentclass{standalone}
% Preamble
\begin{document}

\section{Univariate case}
\label{univariate}
We recall some well-known facts about univariate polynomials;
in this section we consider a polynomial $f = a_0x^d + \dots + a_{d-1}x + a_d \in \mathbb{Q}[x]$ in the variable $x$, with rational coefficients; we denote by $A = \mathbb{Q}[x]/\langle f \rangle$ the quotient algebra of $\mathbb{Q}[x]$ by the ideal $\langle f \rangle$, and we denote indifferently by $x$ the variable $x$, its projection on the quotient algebra $A$ and the multiplication map $x : \left\vert h \mapsto xh \right.$ defined on $A$. 
The special basis $\bold{x} = (1, x,\cdots, x^{d-1})$ of the vector space $A$ is called the {\bf monomial basis}.

\subsection{Mutiplication maps}
The multiplication map $x : \left\vert h \mapsto xh \right.$ is an endomorphism of $A$ which, when written in the monomial basis, has a matrix $X$ called the {\bf companion matrix} of $f$. 
The matrix $X$ is Hessenberg and writes
\begin{equation}
\label{compan}
X =
\begin{bmatrix}
	0 & \cdots & 0 & -a_d/a_0 \\
	1 & 0 & \cdots & -a_{d-1}/a_0 \\
	\vdots  & \ddots  & \ddots & \vdots  \\
	0 & \cdots & 1 & -a_1/a_0
\end{bmatrix}
\end{equation}
\begin{prop}
\label{compan2roots}
The characteristic polynomial of $X$ is $f$.
\end{prop}

\begin{rem}
We deduce from Proposition \ref{compan2roots} that the eigenvalues of $X$ are the roots of $f$, taking account the multiplicities. 
Moreover, as the matrix $X$ is Hessenberg, we can use reliable techniques, like the QR algorithm, to compute its eigenvalues. 
This gives a practical and fast method to compute numerical approximations of the roots of $f$. 
\end{rem}

\begin{rem}
\label{g_1=g_2}
If $g_1, g_2$ are two polynomials of $\mathbb{Q}[x]$ that are equal modulo $f$, then $g_1(X) = g_2(X)$;
therefore, if $g \in A$, then the matrix $g(X)$ is defined without any ambiguity;
this matrix does not depend on the choice of the particular representative of $g$, and it is the matrix of the map $g:\left\vert h \mapsto gh \right.$, written in the monomial basis.
\end{rem}

\subsection{Bezout polynomials and Bezout matrices}
As we have seen, the companion matrix can be used to calculate the roots of a univariate polynomial. 
Interestingly, this can be naturally extended to zero-dimensional multivariate systems; 
if we have $n$ polynomials $f = f_1, \ldots, f_n$ in the variables $x = x_1, \ldots, x_n$, then we simply define the companion matrices as the matrices of the multiplication maps $x_j : \left\vert h \mapsto x_jh \right.$ defined on the quotient algebra $ A = \mathbb{Q}[x]/ \langle f\rangle$, written in some basis of $A$. 
However, in the multivariate case, $A$ has no canonical basis, and the companion matrices do not have an obvious form. 
We can nonetheless resolve this problem by using a family of matrices, the so-called Bezout matrices, that exists both in the univariate case and in the multivariate case, and that serve as intermediate matrices to construct the companion matrices. 
Let's introduce the Bezout matrices in the case of univariate polynomials.

\begin{defn}
\label{def_bez}
Let $f \in \mathbb{Q}[x]$ be a fixed polynomial. Let us introduce a new variable $y$ and let $g$ be any other polynomial. 
The {\bf Bezout polynomial} $\delta(g)$, or {\bf bezoutian}, is defined as the polynomial in the two variables $x, y$
$$
\delta(g) = \dfrac{f(x)g(y)-f(y)g(x)}{x-y}
$$
This polynomial is of degree $m-1$ in both variables $x, y$, where $m$ is the maximum of the degrees of $f$ and $g$.
If we write the bezoutian
\begin{equation}
\delta(g) = \sum_{\alpha,\beta = 0, \cdots, m-1} b_{\alpha\beta} x^\alpha y^\beta
\end{equation}
then the matrix of coefficients $B(g) = [b_{\alpha\beta}]$ is called the {\bf Bezout matrix}.
\end{defn}

\begin{rem}
The size of a Bezout matrix may be loosely defined; when working with several Bezout matrices, it may be desirable to pad some of them with extra columns or lines of zeros to get compatible sizes.
\end{rem}
\begin{rem}
The Bezout poynomial $\delta(g)$ and the Bezout matrix $B(g)$ satisfy the following equality
\begin{equation}
	\label{xBg}
	\delta(g) = \bold{x} B(g) \bold{y}^T
\end{equation}
with $\bold{x} = (1, x,\cdots, x^{m-1}) \in \mathbb{Q}[x]^m$ and $\bold{y} = (1, y,\cdots, y^{m-1}) \in \mathbb{Q}[y]^m$ are two vectors of monomials.
\end{rem}

\begin{exmp}
\label{exmp_1}
We choose $f = x^2 - 3x + 2$ as the fixed polynomial, and we examine the two cases $g=1$ and $g = x^3$. 
The Bezout polynomials are $\delta(1) = -3 + x + y$ and $\delta(x^3) = -2x^2 - 2xy -2y^2 + 3x^2y + 3xy^2 -x^2y^2$. 
The Bezout matrices $B(1)$ et $B(x^3)$ appear when we write  $\delta(1)$ and  $\delta(x^3)$ as double-entry arrays indexed by the monomials $1, x, x^2$ and $1, y, y^2$.
$$
\begin{array}{c|ccc}
\delta(1) & 1 & y & y^2\\
\hline
1 & -3 & 1 & 0\\
x & 1 & 0 & 0\\
x^2 & 0 & 0 & 0
\end{array}
\hspace{1cm}
\begin{array}{c|ccc}
\delta(x^3) & 1 & y & y^2\\
\hline
1 & 0 & 0 & -2\\
x & 0 & -2 & 3\\
x^2 & -2 & 3 & -1
\end{array}
$$
\end{exmp}

\begin{prop}
\label{relations_prop}
Let $f$ be a fixed polynomial and $g$ be another polynomial; if we denote by $m$ the maximum of the degrees of $f$ and $g$ and put $\bold{x} = (1, x,\cdots, x^{m-1})$, then
\begin{equation}
\label{relations}
	\bold{x}B(1)g = \bold{x}B(g)
\end{equation}
where the equality must be understood componentwise in $\mathbb{Q}[x]^m$ and modulo $f$.
\end{prop}
\begin{proof}
We rewrite $\delta(g)$ as
\begin{align*}
	\delta(g) & = & g(x)\dfrac{f(x)-f(y)}{x-y} - f(x)\dfrac{g(x)-g(y)}{x-y} \\ \nonumber
	\delta(g) & = & g(x)\delta(1) - f(x)\dfrac{g(x)-g(y)}{x-y}
\end{align*}
This is an equality between elements of $\mathbb{Q}[x][y]$. 
If $h\in \mathbb{Q}[x][y]$ and $\beta\in\mathbb{N}$, we denote by $h_\beta \in \mathbb{Q}[x]$ the coefficient of $y^\beta$ in the polynomial $h$; then
$$\delta(g)_\beta = g(x)\delta(1)_\beta - f(x)\left(\dfrac{g(x)-g(y)}{x-y}\right)_\beta$$
which holds in $\mathbb{Q}[x]$. Thus, we have $\delta(g)_\beta = g(x)\delta(1)_\beta$ modulo $f$; as this is true for all $\beta\in\mathbb{N}$, Equality (\ref{relations}) follows.
\end{proof}

\begin{rem}
Each column of a Bezout matrix, when left-multiplied by $\bold{x}$, is a polynomial in the variable $x$; when this does not bring to confusion, we will think of columns of a Bezout matrix as elements of $\mathbb{Q}[x]$, and lines as elements of $\mathbb{Q}[y]$.
Saying Proposition \ref{relations_prop} differently : each column of $B(1)$, when multiplied by $g$, equals the column of same index of $B(g)$, modulo $f$.
\end{rem}

\begin{exmp}
Returning to Example \ref{exmp_1}, Proposition \ref{relations_prop} says that the following equalities hold, modulo $f$
\begin{align*}
 (-3 + x)x^3 &= -2x^2, \\
 (1)x^3 &= -2x + 3x^2, \\
 (0)x^3 &= -2 + 3x - x^2
\end{align*}
\end{exmp}

\begin{rem}
If we work with lines instead of columns, then Proposition \ref{relations_prop} says that $gB(1)\bold{y}^T = B(g)\bold{y}^T$, giving equalities in $\mathbb{Q}[y]/\langle f \rangle$.
\end{rem}


\subsection{Relation between Bezout matrices and the companion matrix}
\label{Bar}
Given a fixed polynomial $f$, of degree $d$, the two bezoutians $\delta(1)$ and $\delta(x)$ write
\begin{equation}
	\begin{array}{c|cccc}
		\delta(1) & 1 & y & \dots & y^{d-1} \\
		\hline
		1 & a_{d-1} & \ldots & \dots & a_0 \\
		x & a_{d-2} & \dots & a_0 & 0 \\
		\vdots & \vdots & \vdots & \vdots & \vdots \\
		x_{d-1} & a_0 & 0 & \ldots & 0 \\
	\end{array}
	\hspace{1.5cm}
	\begin{array}{c|cccc}
		\delta(x) & 1 & y & \dots & y^{d-1} \\
		\hline
		1 & -a_{d} & 0 & \dots & 0 \\
		x & 0 & a_{d-2} & \ldots & a_0 \\
		\vdots & \vdots & \vdots & \vdots & \vdots \\
		x_{d-1} & 0 & a_0 & \ldots & 0 \\
	\end{array}
\end{equation}
that make appear the two Bezout matrices $B(1)$, which is clearly invertible, and $B(x)$. 
These two matrices are specially important because they are related to the companion matrix:
\begin{prop}
\label{Barnett}
The compagnon matrix $X$ and the Bezout matrices $B(x), B(1)$ are related by the {\bf Barnett decomposition} formula
\cite{Barnett}
\begin{equation}
	X = B(x)B(1)^{-1}
\end{equation}
\end{prop}

\begin{proof}
Let us consider the two families of elements of the quotient algebra $A$
\begin{equation}
	\begin{array}{lll}
		\bold{x}B(1) & = & (a_{d-1} + a_{d-2}x + \cdots + a_0x^{d-1}, \cdots, a_1 + a_0x,  a_0).\\
		\bold{x}B(x) & = & (-a_d, a_{d-2}x + \cdots + a_0x^{d-1}, \cdots, a_0x)
	\end{array}
\end{equation}
and put $\hat{\bold{x}} = \bold{x}B(1)$.
As $B(1)$ is invertible, the family $\hat{\bold{x}}$ is a basis of the vector space $A$, called the {\bf Horner basis}.
From Proposition \ref{relations_prop}, we have $\hat{\bold{x}}x = \bold{x}B(1)$. 
By construction, $B(1)$ is the matrix of the Horner basis $\hat{\bold{x}}$ written on the monomial basis $\bold{x}$, and $B(x)$ is the matrix of the family $\hat{\bold{x}}x$ written on the monomial basis;
$B(1)^{-1}B(x)$ is thus the matrix of the family $\hat{\bold{x}}x$ written on the Horner basis $\hat{\bold{x}}$. 
This means that the multiplication map $x : \left\vert h \mapsto xh\right.$ is represented in the Horner basis $\hat{\bold{x}}$ by the matrix $B(1)^{-1}B(x)$;
the multiplication map $x$ is also represented in the monomial basis $\bold{x}$ by the matrix $B(1)(B(1)^{-1}B(x))B(1)^{-1} = B(x)B(1)^{-1}$.
\end{proof}

\subsection{Barnett decomposition formula}
\label{Bar_gen}
The Barnett decomposition formula relates the companion matrix, representing the multiplication map $x$, to the Bezout matrices of the polynomials $1$ and $x$;
this can be naturally extended as follows.
Let $g \in \mathbb{Q}[x]$ be any polynomial;
the Bezout matrices $B(1)$ et $B(g)$ are related to the matrix $g(X)$ by the {\bf general Barnett decomposition formula}
\begin{equation}
	\label{GBG}
	B(g)B(1)^{-1} = g(X)
\end{equation}
given that if the sizes of $B(1)$ and $B(g)$ differ, then we must transform and resize $B(g)$ according to a procedure that we shall now explain on the following example.

Formula~(\ref{GBG}) is easily checked when the degree of $g$ is smaller or equal to the degree of $f$, because $B(1)$ and $B(g)$ have the same size;
for example, if $f = x^2 - 3x + 2$, then we have
$$
\begin{array}{c|cc}
	\delta(1) & 1 & y \\
	\hline
	1 & -3 & 1 \\
	x & 1 & 0
\end{array}
\hspace{1cm}
\begin{array}{c|cc}
	\delta(x) & 1 & y \\
	\hline
	1 & -2 & 1 \\
	x & 1 & 0
\end{array}
\hspace{1cm}
\begin{array}{c|cc}
	\delta(x^2) & 1 & y \\
	\hline
	1 & 0 & -2 \\
	x & -2 & 3
\end{array}
$$
and
\begin{equation}
	B(x)B(1)^{-1} =
	\begin{bmatrix}
		0 & -2 \\
		1 & 3
	\end{bmatrix}
	= X
	\hspace{1cm}
	B(x^2)B(1)^{-1} =
	\begin{bmatrix}
		-3 & -6 \\
		2 & 7
	\end{bmatrix}
	= X^2
\end{equation}
which is consistent with formula~(\ref{GBG}).
On the other hand, if $m$, the degree of $g$ is strictly larger than $d$, the degree of $f$, then the sizes of $B(g)$ and $B(1)$ differ, and the product $B(g)B(1)^{-1}$ no longer makes sense. 
This can be fixed by indexing the Bezout matrices by the same monomials, namely $\bold{x} = (1, x,\cdots, x^{m-1})$ and $\bold{y} = (1, y,\cdots, y^{m-1})$. 
For example, with $f$ as above and $g = x^3$, we have
$$
\begin{array}{c|ccc}
\delta(1) & 1 & y & y^2\\
\hline
1 & -3 & 1 & 0\\
x & 1 & 0 & 0\\
x^2 & 0 & 0 & 0
\end{array}
\hspace{1cm}
\begin{array}{c|ccc}
\delta(x^3) & 1 & y & y^2\\
\hline
1 & 0 & 0 & -2\\
x & 0 & -2 & 3\\
x^2 & -2 & 3 & -1
\end{array}
$$
In doing so, $B(1)$ is no longer invertible;
the key to obtain simultaneously matrices of equal size and the invertibility of $B(1)$, is to reduce the bezoutians modulo $f$. 
Let's illustrate this process on the previous example, and write
\begin{align} \nonumber %%%%%%%%%%%%%%
	\delta(x^3) &=
	\begin{bmatrix}
			1 & x & x^2
	\end{bmatrix}
	\begin{bmatrix}
		0 & 0 & -2 \\
		0 & -2 & 3 \\
		-2 & 3 & -1
	\end{bmatrix}
	\begin{bmatrix}
		1 \\
		y \\
		y^2
	\end{bmatrix} \\ \nonumber %%%%%%%%%%%%%%
	\delta(x^3) &=
	\begin{bmatrix}
		1 & x & x^2
	\end{bmatrix}
	\begin{bmatrix}
		1 & 0 & 2 \\
		0 & 1 & -3 \\
		0 & 0 & 1
	\end{bmatrix}
	\begin{bmatrix}
		1 & 0 & -2 \\
		0 & 1 & 3 \\
		0 & 0 & 1
	\end{bmatrix}
	\begin{bmatrix}
		0 & 0 & -2 \\
		0 & -2 & 3 \\
		-2 & 3 & -1
	\end{bmatrix}
	\begin{bmatrix}
		1 \\
		y \\
		y^2
	\end{bmatrix} \\ \nonumber %%%%%%%%%%%%%%%
	\delta(x^3) &=
	\begin{bmatrix}
			1 & x & 2 - 3x + x^2
	\end{bmatrix}
	\begin{bmatrix}
		4 & -6 & 0 \\
		-6 & 7 & 0 \\
		-2 & 3 & -1
	\end{bmatrix}
	\begin{bmatrix}
		1 \\
		y \\
		y^2
	\end{bmatrix} \\ \nonumber %%%%%%%%%%%%%%
\end{align}
To sum up, we have post-multiplied the row vector
$\begin{bmatrix}
	1 & x & x^2
\end{bmatrix}$ by the Gauss transform
$$P =
\begin{bmatrix}
	1 & 0 & 2 \\
	0 & 1 & -3 \\
	0 & 0 & 1
\end{bmatrix}$$
and pre-multiplied the two Bezout matrices $B(1)$ and $B(g)$ by $P^{-1}$. The bezoutians now write
$$
\begin{array}{c|ccc}
	\delta(1) & 1 & y & y^2\\
	\hline
	1 & -3 & 1 & 0\\
	x & 1 & 0 & 0\\
	2 - 3x + x^2 & 0 & 0 & 0
\end{array}
\hspace{1cm}
\begin{array}{c|ccc}
	\delta(x^3) & 1 & y & y^2\\
	\hline
	1 & 4 & -6 & 0 \\
	x & -6 & 7 & 0 \\
	2 - 3x + x^2 & -2 & 3 & -1
\end{array}
$$
According to the relations~(\ref{relations}) the third column of $\delta(x^3)$,  $-2 + 3x - x^2$, is zero modulo $f$; we recognize the simple fact $-f = 0$. Thus,
\begin{align} \nonumber %%%%%%%%%%%%%%
\delta(1) &= \begin{bmatrix}
	1 & x
\end{bmatrix}
\begin{bmatrix}
	-3 & 1 \\
	1 & 0
\end{bmatrix} \nonumber %%%%%%%%%%%%%%
\begin{bmatrix}
	1 \\
	y
\end{bmatrix}\\
\delta(x^3) &= \begin{bmatrix}
	1 & x
\end{bmatrix}
\begin{bmatrix}
	4 & -6 \\
	-6 & 7
\end{bmatrix} \nonumber %%%%%%%%%%%%%%
\begin{bmatrix}
	1 \\
	y
\end{bmatrix} + (2 - 3x + x^2)(-2 + 3y - y^2)
\end{align}
Now, the bezoutians, when reduced modulo $f$ write
$$
\begin{array}{c|cc}
	\delta(1) & 1 & y \\
	\hline
	1 & -3 & 1 \\
	x & 1 & 0
\end{array}
\hspace{1cm}
\begin{array}{c|cc}
	\delta(x^3) & 1 & y \\
	\hline
	1 & 4 & -6  \\
	x & -6 & 7
\end{array}
$$
We have obtained two Bezout matrices of equal size, with $B(1)$ invertible. 
If we compute the matrix ratio
\begin{equation}
	B(x^3)B(1)^{-1} =
	\begin{bmatrix}
		-6 & -14 \\
		7 & 15
	\end{bmatrix}
	= X^3
\end{equation}
then we see that it is consistent with the general Barnett decomposition formula~(\ref{GBG}).

\begin{rem}
Instead of a Gauss matrix, we may use any matrix that maps a given column vector to a column vector containing just one non-zero entry, such as, for example, a Householder orthogonal matrix. This is the choice made in the implementation of the practical method given in \cite{jp_code}.

\end{rem}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "bezout"
%%% End:
