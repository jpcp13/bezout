\documentclass{standalone}
% Preamble
\begin{document}


\subsection{Barnett decomposition formula and structure of the quotient algebra.}
Since the ideal $\langle f\rangle$ is zero-dimensional, the dimension of the quotient algebra $A = \mathbb{Q}[\bold{x}]/\langle f\rangle$ is finite; we may look for some basis and its related companion matrices (matrices of the multiplication maps by $x_1,\cdots, x_n$). 
For this purpose, we will adapt the process described in Section \ref{Bar_gen} but, before, we will specify a number of algebraic properties about polynomial $\delta^0$ and matrices $B^{(k)}$.

\subsubsection{Algebraic properties of Bezout polynomials and Bezout matrices}
The following properties are simple; for a proof, the interested reader may refer to \cite{jpc}. As in Proposition \ref{Barnett}, we define families of elements of $A$ by forming the vector-matrix products
\begin{equation}
		\hat{\bold{x}}_k  =  \bold{x}B^{(k)}, \quad k=0\cdots n
\end{equation}
where $\bold{x}$ is the set of all the monomials $x^\alpha$ appearing in the Bezout polynomials $\delta^{(k)}, \quad k=0\cdots n$. We also write $\hat{\bold{x}}_0 = \hat{\bold{x}}$.
\begin{exmp}
Following Example \ref{bez_multi} we have
\begin{equation}
	\begin{array}{lll}
		\hat{\bold{x}} & = & (0, -x_2 - x_1x_2^2, -1 - x_1x_2, x_1, -x_2, 1) \\
		\hat{\bold{x}}_1 & = & (0, 0, 0, -1 - x_2^2, -x_1x_2, x_1) \\
		\hat{\bold{x}}_2 & = & (-1 - x_1x_2, -x_2 - x_2^2 - x_1, - x_2 - x_1x_2^2, x_1x_2, -x_2^2, x_2)
	\end{array}
\end{equation}
\end{exmp}

\begin{prop}
\label{xj} (see \cite{jpc}).
For all $k=1\cdots n$ we have, modulo $\langle f \rangle$,
\begin{equation}
    \hat{\bold{x}}x_k = \hat{\bold{x}}_k
\end{equation}
\end{prop}
This proposition can be easily checked on Example \ref{bez_multi}. 

So far, the univariate and multivariate cases are very similar, except on one point : in the multivariate case the families $\bold{x}$ and $\hat{\bold{x}}$ are not, in general, bases of the vector space $A$. We have, however, the weaker result (see \cite{jpc}) :

\begin{prop}
Both $\bold{x}$ and $\hat{\bold{x}}$ are generating families of the vector space $A$.
\end{prop}

\subsubsection{Reduction process}
\label{sec:reduction_process}
Following the matrix handlings described in Section \ref{Bar_gen}, we will show how to compute, starting from the generating families $\bold{x}, \hat{\bold{x}}$ and the Bezout matrices $B^{(k)}$, a basis of $A$ and the companion matrices $X_k$.
Let us illustrate this process on Example \ref{bez_multi}.\\
The first column is zero in $B^{(0)}$ but not in $B^{(2)}$; this gives, modulo $\langle f \rangle$, the relation $1 + x_1x_2 = 0$. Then, we right-multiply $\bold{x}$ by the Gauss matrix $P$ whose fifth column is $(1, 0, 0, 0, 1, 0)^{T}$ and left-multiply the Bezout matrices by $P^{-1}$; the Bezout polynomials write 
$$
\begin{array}{c|cccccc}
	\delta^{(0)} & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  &  &  &  & 1\\
	x_2 &  & -1 &  &  & -1 & \\
	x_2^2 &  &  &  &  &  & \\
	x_1 &  &  &  & 1 &  & \\
	1+x_1x_2 &  &  & -1 &  &  & \\
	x_1x_2^2 &  & -1 &  &  &  &
\end{array}$$
$$
\begin{array}{c|cccccc}
	\delta^{(1)} & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  &  & 1 & 1 & \\
	x_2 &  &  &  &  &  & \\
	x_2^2 &  &  &  &  &  & \\
	x_1 &  &  &  &  &  & 1\\
	1+x_1x_2 &  &  &  &  & -1 & \\
	x_1x_2^2 &  &  &  & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccccc}
	\delta^{(2)} & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  &  & -1 &  & \\
	x_2 &  & -1 & -1 &  &  & 1\\
	x_2^2 &  & -1 &  &  & -1 & \\
	x_1 &  & -1 &  &  &  & \\
	1+x_1x_2 & -1 &  &  & 1 &  & \\
	x_1x_2^2 &  &  & -1 &  &  &
\end{array}
$$
Since $1 + x_1x_2 = 0$, modulo $\langle f \rangle$, we can remove the fifth row in the Bezout matrices; the Bezout polynomials write
$$
\begin{array}{c|ccccc}
	\delta^{(0)} & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  &  &  & 1 \\
	x_2  & -1 &  &  & -1 & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  & 1 &  & \\
	x_1x_2^2  & -1 &  &  &  &
\end{array}$$
$$
\begin{array}{c|ccccc}
	\delta^{(1)}  & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & 1 & 1 & \\
	x_2  &  &  &  &  & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  &  &  & 1 \\
	x_1x_2^2  &  &  & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccccc}
	\delta^{(2)} & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & -1 &  & \\
	x_2  & -1 & -1 &  &  & 1 \\
	x_2^2  & -1 &  &  & -1 & \\
	x_1  & -1 &  &  &  & \\
	x_1x_2^2 &  & -1 &  &  &
\end{array}
$$
Now, the second column is zero in $B^{(0)}$ but not in $B^{(2)}$. We have $x_2 + x_1x_2^{2} = 0$, modulo $\langle f \rangle$ . We repeat the previous step with the Gauss matrix $P$ whose fifth column is $(0, 1, 0, 0, 1)^{T}$; the Bezout polynomials write
$$
\begin{array}{c|ccccc}
	\delta^{(0)} & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  &  &  & 1 \\
	x_2  &  &  &  & -1 & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  & 1 &  & \\
	x_2 + x_1x_2^2  & -1 &  &  &  &
\end{array}$$
$$
\begin{array}{c|ccccc}
	\delta^{(1)}  & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & 1 & 1 & \\
	x_2  &  &  & 1 &  & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  &  &  & 1 \\
	x_2 + x_1x_2^2  &  &  & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccccc}
	\delta^{(2)} & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & -1 &  & \\
	x_2  & -1 &  &  &  & 1 \\
	x_2^2  & -1 &  &  & -1 & \\
	x_1  & -1 &  &  &  & \\
	x_2 + x_1x_2^2 &  & -1 &  &  &
\end{array}
$$
Since $x_2 + x_1x_2^{2} = 0$, modulo $\langle f \rangle$, we can remove the fifth row in each Bezout matrix; the Bezout polynomials write
$$
\begin{array}{c|cccc}
	\delta^{(0)} & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &   &  &  & 1 \\
	x_2  &  &  & -1 & \\
	x_2^2  &  &  &  & \\
	x_1  &  & 1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccc}
	\delta^{(1)}  & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & 1 & 1 & \\
	x_2  &  & 1 &  & \\
	x_2^2  &  &  &  & \\
	x_1  &  &  &  & 1
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccc}
	\delta^{(2)} & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & -1 &  & \\
	x_2  & -1 &  &  & 1 \\
	x_2^2  & -1 &  & -1 & \\
	x_1  & -1 &  &  &
\end{array}
$$
Now, the first column is zero in $B^{(0)}$ but not in $B^{(2)}$. We have $x_2 + x_2^{2} + x_1 = 0$, modulo $\langle f \rangle$. We use the Gauss matrix $P$ whose fourth column is $(0, 1, 1, 1)^{T}$; the Bezout polynomials write
$$
\begin{array}{c|cccc}
	\delta^{(0)} & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &   &  &  & 1 \\
	x_2  &  & -1 & -1 & \\
	x_2^2  &  & -1 &  & \\
	x_2 + x_2^{2} + x_1  &  & 1 &  &
\end{array}$$
$$
\begin{array}{c|cccc}
	\delta^{(1)} & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & 1 & 1 & \\
	x_2  &  &  &  & \\
	x_2^2  &  &  &  & -1 \\
	x_2 + x_2^{2} + x_1  &  &  &  & 1
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccc}
	\delta^{(2)} & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & -1 &  & \\
	x_2  &  &  &  & 1 \\
	x_2^2  &  &  & -1 & \\
	x_2 + x_2^{2} + x_1  & -1 &  &  &
\end{array}$$

Since $x_2 + x_2^{2} + x_1 = 0$, modulo $\langle f \rangle$, we can remove the fourth row in each Bezout matrix; the Bezout polynomials write
$$
\begin{array}{c|ccc}
	\delta^{(0)} & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & 1 \\
	x_2  & -1 & -1 & \\
	x_2^2 & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccc}
	\delta^{(1)} & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  & 1 & 1 & \\
	x_2  & 1 &  & -1\\
	x_2^2  &  &  & -1
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccc}
	\delta^{(2)} & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  & -1 &  & \\
	x_2  &  &  & 1 \\
	x_2^2  &  & -1 &
\end{array}$$
Matrix $B^{(0)}$ is now invertible; the reduction process is completed. The dimension of $A$ is~$3$. We observe that $\bold{x} = (1, x_2, x_2^{2})$ and $\bold{y} = (y_1, y_1^{2}, y_1^{3})$ are bases of $A$; the associated Horner bases are $\hat{\bold{x}} = (-x_2-x_2^{2}, -x_{2}, 1)$ and $\hat{\bold{y}} = (y_1^{3}, -y_1^{2}-y_1^{2}y_2, -y_1^{2})$.
More generally we have (\cite{jpc} p.57, \cite{bm}, \cite{tm})
\begin{prop}
\label{conjecture}
After the reduction process described above is completed, that is to say when $B^{(0)}$ is invertible and all the matrices $B^{(k)}, k=0, \cdots, n$ have the same size, then the indexing families $\bold{x}$ and $\bold{y}$ are bases of $A$.
\end{prop}

\begin{rem}
Proposition \ref{conjecture} is guaranted when the ideal is zero-dimensional; in this case, to complete the reduction process we just have to use zero-columns of $B^{(0)}$ or, more generally, linear combinations of columns that vanish, i.e elements of the right kernel of $B^{(0)}$.
\end{rem}

\subsubsection{Bezout matrices are related to the companion matrices}
As in Proposition \ref{Barnett}, we consider the matrices $X_1, X_2$ defined by
\begin{equation}
	X_1 = B^{(1)}{B^{(0)}}^{-1} =
	\begin{bmatrix}
		0 & -1 & 0\\
		-1 & 0 & -1\\
		-1 & 0 & 0
	\end{bmatrix},\quad
	X_2 = B^{(2)}{B^{(0)}}^{-1} =
	\begin{bmatrix}
		0 & 0 & 1\\
		1 & 0 & 0\\
		0 & 1 & -1
	\end{bmatrix}
\end{equation}
We can see that $X_1, X_2$ are the multiplication matrices by the variables $x_1, x_2$ in the basis $\bold{x}$, i.e the companion matrices associated to the basis $\bold{x}$. More generally, we have
\begin{prop}
\label{Barnett_multi}
After the reduction process has been completed, the companion matrices $X_j$ and the Bezout matrices are related by the {\bf Barnett formulas}
\begin{equation}
	X_j = B^{(j)}{B^{(0)}}^{-1}, j = 1,\cdots, n
\end{equation}
\end{prop}

\begin{rem}
As in the univariate case, we have, for all $j=1,\cdots,n$,\\
$B^{(j)}{B^{(0)}}^{-1}$ is the multiplication matrix by $x_j$ in the basis $\bold{x}$ \\
${B^{(j)}}^{T}{B^{(0)}}^{-T}$ is the multiplication matrix by $y_j$ in the basis $\bold{y}$ \\
${B^{(0)}}^{-1}{B^{(j)}}$  is the multiplication matrix by $x_j$ in the basis $\hat{\bold{x}}$ \\
${B^{(0)}}^{-T}{B^{(j)}}^{T}$  is the multiplication matrix by $y_j$ in the basis $\hat{\bold{y}}$
\end{rem}

\subsubsection{Numerical computation of the roots}
As in the univariate case, (see Proposition \ref{compan2roots}), the roots of the polynomial system$f_1, \cdots, f_n$ are the eigenvalues of the companion matrices (\cite{AS}). The eigenvalues of matrices $X_1, X_2$ above are
$$
\begin{array}{c|c}
	x_1 & x_2 \\
	\hline
	-1.32472  & 0.75488 \\
	0.66236 + 0.56228i & -0.87744 + 0.74486i \\
	0.66236 - 0.56228i & -0.87744 - 0.74486i
\end{array}
$$
Since $A$ is a commutative algebra, the matrices $X_1, X_2$ commute and have the same eigenvectors. We must be careful to sort the eigenvalues of $X_1, X_2$ so that they correspond to the same eigenvectors. In this example, it is easy to check that the couples $(x_1, x_2)$ are numerical approximations of the roots of the polynomial system $f_1 = x_1^2 + x_1x_2^2 - 1, f_2 = x_1^2x_2 + x_1$ defined in Example \ref{bez_multi}.


\end{document}
