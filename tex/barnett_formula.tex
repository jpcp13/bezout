\documentclass{standalone}
% Preamble
\begin{document}


\subsection{Barnett decomposition formula and structure of the quotient algebra.}
Since the ideal $\langle f\rangle$ is zero-dimensional, the dimension of the quotient algebra $A = \mathbb{Q}[\bold{x}]/\langle f\rangle$ is finite; we may look for some basis and its related companion matrices (matrices of the multiplication maps by $x_1,\cdots, x_n$). 
For this purpose, we will adapt the process described in Section \ref{Bar_gen} but, before, we will specify a number of algebraic properties about the polynomial $\delta^0$ and matrices $B^{(k)}$.
\subsubsection{Algebraic properties of polynomial $\delta^0$ and matrix $B^{(0)}$}
The following properties are simple; for a proof, the interested reader may refer to \cite{jpc}. As in Proposition \ref{Barnett}, we define families of elements of $A$ by forming the vector-matrix products
\begin{equation}
		\hat{\bold{x}}_k  =  \bold{x}B^{(k)}, \quad k=0\cdots n
\end{equation}
with the convention that $x_0=1$ and where $\bold{x}$ is the set of all the monomials $x^\alpha$ appearing in the Bezout polynomials $\delta^{(k)}, \quad k=0\cdots n$. We also write $\hat{\bold{x}}_0 = \hat{\bold{x}}$.
\begin{exmp}
Following Example \ref{bez_multi} we have
\begin{equation}
	\begin{array}{lll}
		\hat{\bold{x}} & = & (0, -x_2 - x_1x_2^2, -1 - x_1x_2, x_1, -x_2, 1) \\
		\hat{\bold{x}}_1 & = & (0, 0, 0, -1 - x_2^2, -x_1x_2, x_1) \\
		\hat{\bold{x}}_2 & = & (-1 - x_1x_2, -x_2 - x_2^2 - x_1, - x_2 - x_1x_2^2, x_1x_2, -x_2^2, x_2)
	\end{array}
\end{equation}
\end{exmp}

\begin{prop}
\label{xj} (see \cite{jpc}).
For all $k=1\cdots n$ we have
\begin{equation}
    \hat{\bold{x}}x_k = \hat{\bold{x}}_k
\end{equation}
\end{prop}
This proposition can be easily checked on Example \ref{bez_multi}. 
So far, the univariate and multivariate cases are very similar, except on one point : in the multivariate case the families $\bold{x}$ and $\hat{\bold{x}}$ are not, in general, bases of the vector space $A$. We have, however, the weaker result (see \cite{jpc}) :

\begin{prop}
Both $\bold{x}$ and $\hat{\bold{x}}$ are generating families of the vector space $A$.
\end{prop}

\subsubsection{Reduction process}
\label{sec:reduction_process}
Following the matrix handlings described in Section \ref{Bar_gen}, we now show how to compute, starting from the generating families $\bold{x}, \hat{\bold{x}}$ and the Bezout matrices $B^{(k)}$, a basis of $A$ and the companion matrices $X_k$.
Let us illustrate this process on Example \ref{bez_multi}.\\
The first column of $B^{(0)}$ is zero but that of de $B^{(2)}$ is not; this gives the relation $1 + x_1x_2 = 0$, modulo $I$. Then, we right-multiply $\bold{x}$ by the Gauss matrix $P$ whose $5$th column is $(1, 0, 0, 0, 1, 0)^{T}$ and left-multiply the Bezout matrices by $P^{-1}$; the Bezout polynomials write 
$$
\begin{array}{c|cccccc}
	\delta^{(0)} & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  &  &  &  & 1\\
	x_2 &  & -1 &  &  & -1 & \\
	x_2^2 &  &  &  &  &  & \\
	x_1 &  &  &  & 1 &  & \\
	1+x_1x_2 &  &  & -1 &  &  & \\
	x_1x_2^2 &  & -1 &  &  &  &
\end{array}$$
$$
\begin{array}{c|cccccc}
	\delta^{(1)} & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  &  & 1 & 1 & \\
	x_2 &  &  &  &  &  & \\
	x_2^2 &  &  &  &  &  & \\
	x_1 &  &  &  &  &  & 1\\
	1+x_1x_2 &  &  &  &  & -1 & \\
	x_1x_2^2 &  &  &  & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccccc}
	\delta^{(2)} & 1 & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1 &  &  &  & -1 &  & \\
	x_2 &  & -1 & -1 &  &  & 1\\
	x_2^2 &  & -1 &  &  & -1 & \\
	x_1 &  & -1 &  &  &  & \\
	1+x_1x_2 & -1 &  &  & 1 &  & \\
	x_1x_2^2 &  &  & -1 &  &  &
\end{array}
$$
As we have $1 + x_1x_2 = 0$ we remove the first column and the fifth row in the Bezout matrices; the bezoutians write
$$
\begin{array}{c|ccccc}
	B(1) & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  &  &  & 1 \\
	x_2  & -1 &  &  & -1 & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  & 1 &  & \\
	x_1x_2^2  & -1 &  &  &  &
\end{array}$$
$$
\begin{array}{c|ccccc}
	B(x_1)  & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & 1 & 1 & \\
	x_2  &  &  &  &  & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  &  &  & 1 \\
	x_1x_2^2  &  &  & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccccc}
	B(x_2) & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & -1 &  & \\
	x_2  & -1 & -1 &  &  & 1 \\
	x_2^2  & -1 &  &  & -1 & \\
	x_1  & -1 &  &  &  & \\
	x_1x_2^2 &  & -1 &  &  &
\end{array}
$$
The second column of $B(1)$ is zero but that of $B(x_2)$ is not. This implies that $x_2 + x_1x_2^{2} = 0$. We repeat the previous step with the Gauss matrix $P$ whose fifth column is $(0, 1, 0, 0, 1)^{T}$; the bezoutians write
$$
\begin{array}{c|ccccc}
	B(1) & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  &  &  & 1 \\
	x_2  &  &  &  & -1 & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  & 1 &  & \\
	x_2 + x_1x_2^2  & -1 &  &  &  &
\end{array}$$
$$
\begin{array}{c|ccccc}
	B(x_1)  & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & 1 & 1 & \\
	x_2  &  &  & 1 &  & \\
	x_2^2  &  &  &  &  & \\
	x_1  &  &  &  &  & 1 \\
	x_2 + x_1x_2^2  &  &  & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccccc}
	B(x_2) & y_1 & y_1y_2 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & -1 &  & \\
	x_2  & -1 &  &  &  & 1 \\
	x_2^2  & -1 &  &  & -1 & \\
	x_1  & -1 &  &  &  & \\
	x_2 + x_1x_2^2 &  & -1 &  &  &
\end{array}
$$
As we heve $x_2 + x_1x_2^{2} = 0$, we can remove the second column and the fifth row in each Bezout matrix; the bezoutians write
$$
\begin{array}{c|cccc}
	B(1) & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &   &  &  & 1 \\
	x_2  &  &  & -1 & \\
	x_2^2  &  &  &  & \\
	x_1  &  & 1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccc}
	B(x_1)  & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & 1 & 1 & \\
	x_2  &  & 1 &  & \\
	x_2^2  &  &  &  & \\
	x_1  &  &  &  & 1
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccc}
	B(x_2) & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & -1 &  & \\
	x_2  & -1 &  &  & 1 \\
	x_2^2  & -1 &  & -1 & \\
	x_1  & -1 &  &  &
\end{array}
$$
The first column of $B(1)$ is zero but that of $B(x_2)$ is not. This implies taht $x_2 + x_2^{2} + x_1 = 0$. The new Gauss matrix is $P$ whose fourth column is $(0, 1, 1, 1)^{T}$; the bezoutians write
$$
\begin{array}{c|cccc}
	B(1) & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &   &  &  & 1 \\
	x_2  &  & -1 & -1 & \\
	x_2^2  &  & -1 &  & \\
	x_2 + x_2^{2} + x_1  &  & 1 &  &
\end{array}$$
$$
\begin{array}{c|cccc}
	B(x_1)  & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & 1 & 1 & \\
	x_2  &  &  &  & \\
	x_2^2  &  &  &  & -1 \\
	x_2 + x_2^{2} + x_1  &  &  &  & 1
\end{array}
\hspace{0.2cm}
\begin{array}{c|cccc}
	B(x_2) & y_1 & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  & -1 &  & \\
	x_2  &  &  &  & 1 \\
	x_2^2  &  &  & -1 & \\
	x_2 + x_2^{2} + x_1  & -1 &  &  &
\end{array}$$

As $x_2 + x_2^{2} + x_1 = 0$, we remove the first column eand fourth row in each Bezout matrix; the bezoutians write
$$
\begin{array}{c|ccc}
	B(1) & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  &  &  & 1 \\
	x_2  & -1 & -1 & \\
	x_2^2 & -1 &  &
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccc}
	B(x_1) & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  & 1 & 1 & \\
	x_2  & 1 &  & -1\\
	x_2^2  &  &  & -1
\end{array}
\hspace{0.2cm}
\begin{array}{c|ccc}
	B(x_2) & y_1^2 & y_1^2y_2 & y_1^3 \\
	\hline
	1  & -1 &  & \\
	x_2  &  &  & 1 \\
	x_2^2  &  & -1 &
\end{array}$$
Matrix $B(1)$ is now invertible; the reduction process is completed. The dimension of $A$ is~$3$. We observe that $\bold{x} = (1, x_2, x_2^{2})$ et $\bold{y} = (y_1, y_1^{2}, y_1^{3})$ are bases of $A$; the associated Horner bases are $\hat{\bold{x}} = (-x_2-x_2^{2}, -x_{2}, 1)$ and $\hat{\bold{y}} = (y_1^{3}, -y_1^{2}-y_1^{2}y_2, -y_1^{2})$.
More generally we have (\cite{jpc} p.57, \cite{bm}, \cite{tm})
\begin{prop}
\label{conjecture}
After the reduction process described above is completed, that is to say when $B(1)$ is invertible and all the matrices $B(x_k), k=0, \cdots, n$ have the same size and are indexed by the same families $\bold{x, y}$, then each family $\bold{x, y}$ is a basis of $A$.
\end{prop}

\begin{rem}
Proposition \ref{conjecture} is guaranted only when the ideal is zero-dimensional; in this case, to complete the reduction process we just have to use zero-columns of $B(1)$ or, more generally, linear combinations of columns that vanish, i.e elements of the right kernel of $B(1)$. If, however, the ideal is not zero-dimensional, then our experiments show that the reduction process, using both the right-kernel and the left-kernel of $B(1)$, generally produces an interesting result.
\end{rem}

\subsubsection{Barnett formula and companion matrices}
Following Example \ref{bez_multi}, we define the matrices $X_1, X_2$
\begin{equation}
	X_1 = B(x_1)B(1)^{-1} =
	\begin{bmatrix}
		0 & -1 & 0\\
		-1 & 0 & -1\\
		-1 & 0 & 0
	\end{bmatrix},\quad
	X_2 = B(x_2)B(1)^{-1} =
	\begin{bmatrix}
		0 & 0 & 1\\
		1 & 0 & 0\\
		0 & 1 & -1
	\end{bmatrix}
\end{equation}
We see that $X_1, X_2$ are the multiplication matrices by the variables $x_1, x_2$ in the basis $\bold{x}$; these are the companion matrices associated to the basis $\bold{x}$. More generally, we have
\begin{prop}
\label{Barnett_multi}
When the reduction process has been completed and we have at our disposal Bezout matrices $B(x_j)$ and bases $\bold{x, y}$, the companion matrices $X_j$, i.e the multiplication matrices by the variables $x_1, x_2$ in the basis $\bold{x}$, can be calculated by the {\bf Barnett formulas}
\begin{equation}
	X_j = B(x_j)B(1)^{-1}
\end{equation}
\end{prop}

\begin{rem}
As in the univariate case, we have, for all $j=1,\cdots,n$,\\
$B(x_j)^{T}B(1)^{-T}$ is the multiplication matrix by $y_j$ in the basis $\bold{y}$ \\
$B(1)^{-1}B(x_j)$  is the multiplication matrix by $x_j$ in the basis $\hat{\bold{x}}$ \\
$B(1)^{-T}B(x_j)^{T}$  is the multiplication matrix by $y_j$ in the basis $\hat{\bold{y}}$
\end{rem}

\subsubsection{Numerical computation of the roots}
As in the univariate case, (see Proposition \ref{compan2roots}), the roots of the polynomial system$f_1, \cdots, f_n$ are the eigenvalues of the companion matrices (\cite{AS}).

In Example \ref{bez_multi},the eigenvalues of matrices $X_1, X_2$ are
$$
\begin{array}{c|c}
	x_1 & x_2 \\
	\hline
	-1.32472  & 0.75488 \\
	0.66236 + 0.56228i & -0.87744 + 0.74486i \\
	0.66236 - 0.56228i & -0.87744 - 0.74486i
\end{array}
$$

Since $A$ is a commutative algebra, the matrices $X_1, X_2$ commute and have the same eigenvectors. We must be careful to sort the eigenvalues of $X_1, X_2$ so that they correspond to the same eigenvectors. In Example \ref{bez_multi}, it is easy to check that the couples $(x_1, x_2)$ are numerical approximations of the roots of the polynomial system $f_1 = x_1^2 + x_1x_2^2 - 1, f_2 = x_1^2x_2 + x_1$.


\end{document}
